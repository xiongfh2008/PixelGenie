
import { GoogleGenAI, Modality, Type } from "@google/genai";
import { AnalysisData, Language, TranslationData, TextBlock } from "../types";

// Initialize Gemini Client
const ai = new GoogleGenAI({ apiKey: import.meta.env.VITE_API_KEY });

/**
 * Helper to convert File to Base64 string (without data URI prefix)
 */
export const fileToBase64 = (file: File): Promise<string> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onload = () => {
      const result = reader.result as string;
      // Remove the "data:image/xyz;base64," prefix
      const base64 = result.split(',')[1];
      resolve(base64);
    };
    reader.onerror = (error) => reject(error);
    reader.readAsDataURL(file);
  });
};

/**
 * Helper to fetch an image from a URL and convert it to a File object
 */
export const fetchImageFromUrl = async (url: string): Promise<File> => {
  try {
    const response = await fetch(url);
    if (!response.ok) throw new Error(`Failed to fetch image: ${response.statusText}`);
    
    const blob = await response.blob();
    const contentType = response.headers.get("content-type") || "image/jpeg";
    
    // Extract filename from URL or default
    let filename = "image_from_url.jpg";
    try {
      const urlPath = new URL(url).pathname;
      const name = urlPath.substring(urlPath.lastIndexOf('/') + 1);
      if (name) filename = name;
    } catch (e) {}

    return new File([blob], filename, { type: contentType });
  } catch (error: any) {
    // Common CORS error handling
    if (error.message && error.message.includes('Failed to fetch')) {
      throw new Error("CORS Error: Unable to load image directly due to browser security. Please download the image and upload it manually.");
    }
    throw error;
  }
};

/**
 * Helper: Resize image to ensure it fits within Gemini's processing limits and optimizes speed.
 * Downscales to max 800px (was 1024px) and uses aggressive JPEG compression.
 */
const optimizeImageForApi = (base64: string, mimeType: string): Promise<string> => {
  return new Promise((resolve, reject) => {
    const img = new Image();
    img.src = `data:${mimeType};base64,${base64}`;
    
    img.onload = () => {
      const MAX_SIZE = 800; // Reduced from 1024 for faster upload/tokenization
      let width = img.width;
      let height = img.height;

      // If image is small enough, return original
      if (width <= MAX_SIZE && height <= MAX_SIZE) {
        resolve(base64);
        return;
      }

      // Calculate new dimensions
      if (width > height) {
        if (width > MAX_SIZE) {
          height = Math.round(height * (MAX_SIZE / width));
          width = MAX_SIZE;
        }
      } else {
        if (height > MAX_SIZE) {
          width = Math.round(width * (MAX_SIZE / height));
          height = MAX_SIZE;
        }
      }

      const canvas = document.createElement('canvas');
      canvas.width = width;
      canvas.height = height;
      const ctx = canvas.getContext('2d');
      if (!ctx) {
         reject(new Error("Canvas context unavailable"));
         return;
      }
      
      try {
        // Draw and compress
        ctx.drawImage(img, 0, 0, width, height);
        
        // Use JPEG 0.70 for speed. Visual artifacts from compression are acceptable 
        // as the model looks at semantic consistency mainly.
        const newDataUrl = canvas.toDataURL('image/jpeg', 0.70);
        resolve(newDataUrl.split(',')[1]);
      } catch (e) {
        reject(new Error("Failed to process image canvas"));
      }
    };
    img.onerror = (err) => reject(err);
  });
};

/**
 * Local Self-Implemented AI Metadata Scanner
 * Scans the raw file header for common AI generator signatures.
 */
export const scanForAIMetadata = async (file: File): Promise<{ detected: boolean, tool?: string }> => {
  return new Promise((resolve) => {
    const reader = new FileReader();
    reader.onload = (e) => {
      const result = e.target?.result;
      if (typeof result === 'string') {
         // Common signatures in Exif/XMP/Text chunks
         if (result.includes("Stable Diffusion") || result.includes("sd-webui")) resolve({ detected: true, tool: "Stable Diffusion" });
         else if (result.includes("Midjourney")) resolve({ detected: true, tool: "Midjourney" });
         else if (result.includes("ComfyUI")) resolve({ detected: true, tool: "ComfyUI" });
         else if (result.includes("DALL-E")) resolve({ detected: true, tool: "DALL-E" });
         else if (result.includes("Adobe Firefly")) resolve({ detected: true, tool: "Adobe Firefly" });
         else if (result.includes("Generated by AI")) resolve({ detected: true, tool: "Generic AI" });
         else resolve({ detected: false });
      } else {
          resolve({ detected: false });
      }
    };
    // Read first 50KB. Most metadata headers are at the start.
    // Using ISO-8859-1 preserves bytes 1:1 in the string for searching.
    const blob = file.slice(0, 50 * 1024); 
    reader.readAsText(blob, 'ISO-8859-1'); 
  });
};

/**
 * Detects text blocks and translates them, returning bounding box coordinates.
 */
export const detectTextAndTranslate = async (
  base64: string,
  mimeType: string,
  targetLang: string
): Promise<TranslationData> => {
  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: {
      parts: [
        { inlineData: { mimeType, data: base64 } },
        { text: `Detect all visible text in this image. Translate each text block to ${targetLang}.
                 Return the original text, translated text, and the 2D bounding box [ymin, xmin, ymax, xmax] (0-1000 scale) for each block.` }
      ]
    },
    config: {
      responseMimeType: "application/json",
      responseSchema: {
        type: Type.OBJECT,
        properties: {
          detected_language: { type: Type.STRING },
          original_text: { type: Type.STRING, description: "Full original text concatenated" },
          translated_text: { type: Type.STRING, description: "Full translated text concatenated" },
          blocks: {
            type: Type.ARRAY,
            items: {
              type: Type.OBJECT,
              properties: {
                original: { type: Type.STRING },
                translated: { type: Type.STRING },
                box_2d: {
                  type: Type.ARRAY,
                  items: { type: Type.NUMBER },
                  description: "ymin, xmin, ymax, xmax"
                }
              }
            }
          }
        }
      }
    }
  });

  const text = response.text;
  if (!text) throw new Error("No response from translation service");
  return JSON.parse(text) as TranslationData;
};

/**
 * Analyze an image using Gemini 2.5 Flash.
 * OPTIMIZED FOR SPEED & ACCURACY.
 */
export const analyzeImage = async (
  originalBase64: string, 
  elaBase64: string,
  mfrBase64: string | null, 
  mimeType: string, 
  lang: Language
): Promise<AnalysisData> => {
  try {
    const langMap: Record<Language, string> = {
      en: "English",
      zh: "Simplified Chinese (zh-CN)",
      es: "Spanish",
      ja: "Japanese",
      fr: "French",
      de: "German",
      pt: "Portuguese"
    };

    const targetLang = langMap[lang] || "English";
    const langInstruction = `The final JSON output values MUST be in ${targetLang}.`;

    // Optimize the original image for payload size before sending to LLM
    // This significantly speeds up the request
    const fastBase64 = await optimizeImageForApi(originalBase64, mimeType);

    const parts: any[] = [
      { inlineData: { mimeType: "image/jpeg", data: fastBase64 } },
      { inlineData: { mimeType: 'image/png', data: elaBase64 } }
    ];
    if (mfrBase64) parts.push({ inlineData: { mimeType: 'image/png', data: mfrBase64 } });

    parts.push({
      text: `You are a Lead Digital Forensic Analyst specializing in detecting AI-generated media (Flux, Midjourney v6, Sora) and advanced Photoshop manipulation.
      
      **INPUTS**:
      1. **Original Image**
      2. **ELA Map** (Rainbow): Error Level Analysis. Shows compression discrepancies.
      3. **MFR Map** (Grayscale): Noise Analysis. Authentic photos have uniform noise. AI often has 'black voids' (no noise).

      **EXECUTION PROTOCOL**:
      
      **PHASE 1: SEMANTIC & PHYSICS CHECK (Primary Detection Method)**
      *Scan the Original Image closely.*
      - **AI Artifacts**: Look for glossy/waxy skin texture, perfect symmetry, melded fingers, nonsensical background text, floating objects, or impossible lighting.
      - **Logic**: Do shadows match the light source? Are reflections correct?
      - **Verdict Hint**: If it looks "too perfect" or has "dream-like" physics -> Likely AI.

      **PHASE 2: FORENSIC MAP CONFIRMATION**
      - **ELA (Rainbow)**:
        - **IGNORE** white/rainbow edges on high-contrast lines (text, sharp borders). This is normal JPEG behavior.
        - **FLAG** if a specific object (e.g., a face) is purple while the body is blue. This indicates SPLICING.
      - **MFR (Grayscale)**:
        - **Authentic**: Uniform grain/static across the whole image.
        - **AI Generated**: Often shows smooth black areas (voids) where texture should be, lacking camera sensor noise.

      ${langInstruction}

      **DECISION RULES**:
      1. **AI Generated**: Visually flawless but "plastic" look OR MFR shows lack of noise (black voids) + ELA is uniform.
      2. **Tampered/Spliced**: ELA shows distinct colored block on an object.
      3. **Authentic**: Natural imperfections, consistent noise, consistent ELA (except edges).

      Return the analysis in JSON.`
    });

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: { parts },
      config: {
        temperature: 0.1, // Low temperature for factual analysis
        // Speed Optimization: Reduced budget to 1024. 
        // Sufficient for "Check 1, Check 2, Verdict" logic without over-thinking.
        thinkingConfig: { thinkingBudget: 1024 }, 
        maxOutputTokens: 4096,
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            description: { type: Type.STRING },
            tags: { type: Type.ARRAY, items: { type: Type.STRING } },
            objects: { type: Type.ARRAY, items: { type: Type.STRING } },
            sentiment: { type: Type.STRING },
            colors: { type: Type.ARRAY, items: { type: Type.STRING } },
            integrity: {
              type: Type.OBJECT,
              properties: {
                is_suspected_fake: { type: Type.BOOLEAN },
                confidence_score: { type: Type.NUMBER },
                reasoning: { type: Type.STRING },
                methods_analyzed: { type: Type.ARRAY, items: { type: Type.STRING } },
                ai_generated_probability: { type: Type.NUMBER },
                ai_analysis: {
                  type: Type.OBJECT,
                  properties: {
                    unnatural_textures: { type: Type.BOOLEAN },
                    inconsistent_lighting: { type: Type.BOOLEAN },
                    semantic_inconsistencies: { type: Type.BOOLEAN }
                  }
                }
              }
            }
          }
        }
      }
    });

    const text = response.text;
    if (!text) throw new Error("No response from model");
    return JSON.parse(text) as AnalysisData;

  } catch (error) {
    console.error("Gemini Analysis Failed:", error);
    throw error;
  }
};

/**
 * Edit/Modify an image using Gemini 2.5 Flash Image Model
 */
export const modifyImage = async (
  base64: string | null, 
  mimeType: string | null, 
  prompt: string
): Promise<string> => {
  try {
    const parts: any[] = [];

    if (base64 && mimeType) {
       const optimizedBase64 = await optimizeImageForApi(base64, mimeType);
       parts.push({
         inlineData: {
           mimeType: 'image/jpeg',
           data: optimizedBase64,
         },
       });
    }

    parts.push({ text: prompt });

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image',
      contents: { parts },
    });

    const candidates = response.candidates;
    if (candidates && candidates.length > 0) {
      const parts = candidates[0].content.parts;
      for (const part of parts) {
        if (part.inlineData && part.inlineData.data) {
          return part.inlineData.data;
        }
      }
    }
    
    throw new Error("No image generated in response");
  } catch (error) {
    console.error("Image Modification Failed:", error);
    throw error;
  }
};

/**
 * OCR - Extract Text from Image
 */
export const translateImageText = async (base64: string, mimeType: string, targetLang: string): Promise<TranslationData> => {
  try {
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: {
        parts: [
          { inlineData: { mimeType, data: base64 } },
          { text: `Perform High-Precision OCR. Transcribe and translate to ${targetLang}.` }
        ]
      },
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            detected_language: { type: Type.STRING },
            original_text: { type: Type.STRING },
            translated_text: { type: Type.STRING },
          }
        }
      }
    });

    const text = response.text;
    if (!text) throw new Error("No response");
    return JSON.parse(text) as TranslationData;

  } catch (error) {
    console.error("Translation Failed:", error);
    throw error;
  }
};
