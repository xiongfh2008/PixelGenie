
import { GoogleGenAI, Modality, Type } from "@google/genai";
import { AnalysisData, Language, TranslationData, TextBlock } from "../types";

// Initialize Gemini Client
const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

/**
 * Helper to convert File to Base64 string (without data URI prefix)
 */
export const fileToBase64 = (file: File): Promise<string> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onload = () => {
      const result = reader.result as string;
      // Remove the "data:image/xyz;base64," prefix
      const base64 = result.split(',')[1];
      resolve(base64);
    };
    reader.onerror = (error) => reject(error);
    reader.readAsDataURL(file);
  });
};

/**
 * Helper to fetch an image from a URL and convert it to a File object
 */
export const fetchImageFromUrl = async (url: string): Promise<File> => {
  try {
    const response = await fetch(url);
    if (!response.ok) throw new Error(`Failed to fetch image: ${response.statusText}`);
    
    const blob = await response.blob();
    const contentType = response.headers.get("content-type") || "image/jpeg";
    
    // Extract filename from URL or default
    let filename = "image_from_url.jpg";
    try {
      const urlPath = new URL(url).pathname;
      const name = urlPath.substring(urlPath.lastIndexOf('/') + 1);
      if (name) filename = name;
    } catch (e) {}

    return new File([blob], filename, { type: contentType });
  } catch (error: any) {
    // Common CORS error handling
    if (error.message && error.message.includes('Failed to fetch')) {
      throw new Error("CORS Error: Unable to load image directly due to browser security. Please download the image and upload it manually.");
    }
    throw error;
  }
};

/**
 * Helper: Resize image to ensure it fits within Gemini's processing limits (avoids 500 errors)
 * Downscales to max 1024px dimension and uses JPEG compression.
 */
const optimizeImageForApi = (base64: string, mimeType: string): Promise<string> => {
  return new Promise((resolve, reject) => {
    const img = new Image();
    img.src = `data:${mimeType};base64,${base64}`;
    
    img.onload = () => {
      const MAX_SIZE = 1024; // Safe limit for GenAI image editing to prevent timeouts
      let width = img.width;
      let height = img.height;

      // If image is small enough, return original
      if (width <= MAX_SIZE && height <= MAX_SIZE) {
        resolve(base64);
        return;
      }

      // Calculate new dimensions
      if (width > height) {
        if (width > MAX_SIZE) {
          height = Math.round(height * (MAX_SIZE / width));
          width = MAX_SIZE;
        }
      } else {
        if (height > MAX_SIZE) {
          width = Math.round(width * (MAX_SIZE / height));
          height = MAX_SIZE;
        }
      }

      const canvas = document.createElement('canvas');
      canvas.width = width;
      canvas.height = height;
      const ctx = canvas.getContext('2d');
      if (!ctx) {
         reject(new Error("Canvas context unavailable"));
         return;
      }
      
      try {
        // Draw and compress
        ctx.drawImage(img, 0, 0, width, height);
        
        // Use JPEG 0.85 for optimal balance of quality and payload size
        // This drastic reduction in payload size prevents the XHR Error / RPC Failed 500
        const newDataUrl = canvas.toDataURL('image/jpeg', 0.85);
        resolve(newDataUrl.split(',')[1]);
      } catch (e) {
        reject(new Error("Failed to process image canvas"));
      }
    };
    img.onerror = (err) => reject(err);
  });
};

/**
 * Local Self-Implemented AI Metadata Scanner
 * Scans the raw file header for common AI generator signatures.
 */
export const scanForAIMetadata = async (file: File): Promise<{ detected: boolean, tool?: string }> => {
  return new Promise((resolve) => {
    const reader = new FileReader();
    reader.onload = (e) => {
      const result = e.target?.result;
      if (typeof result === 'string') {
         // Common signatures in Exif/XMP/Text chunks
         if (result.includes("Stable Diffusion") || result.includes("sd-webui")) resolve({ detected: true, tool: "Stable Diffusion" });
         else if (result.includes("Midjourney")) resolve({ detected: true, tool: "Midjourney" });
         else if (result.includes("ComfyUI")) resolve({ detected: true, tool: "ComfyUI" });
         else if (result.includes("DALL-E")) resolve({ detected: true, tool: "DALL-E" });
         else if (result.includes("Adobe Firefly")) resolve({ detected: true, tool: "Adobe Firefly" });
         else if (result.includes("Generated by AI")) resolve({ detected: true, tool: "Generic AI" });
         else resolve({ detected: false });
      } else {
          resolve({ detected: false });
      }
    };
    // Read first 50KB. Most metadata headers are at the start.
    // Using ISO-8859-1 preserves bytes 1:1 in the string for searching.
    const blob = file.slice(0, 50 * 1024); 
    reader.readAsText(blob, 'ISO-8859-1'); 
  });
};

/**
 * Detects text blocks and translates them, returning bounding box coordinates.
 * Used for self-implemented canvas rendering.
 */
export const detectTextAndTranslate = async (
  base64: string,
  mimeType: string,
  targetLang: string
): Promise<TranslationData> => {
  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: {
      parts: [
        { inlineData: { mimeType, data: base64 } },
        { text: `Detect all visible text in this image. Translate each text block to ${targetLang}.
                 Return the original text, translated text, and the 2D bounding box [ymin, xmin, ymax, xmax] (0-1000 scale) for each block.` }
      ]
    },
    config: {
      responseMimeType: "application/json",
      responseSchema: {
        type: Type.OBJECT,
        properties: {
          detected_language: { type: Type.STRING },
          original_text: { type: Type.STRING, description: "Full original text concatenated" },
          translated_text: { type: Type.STRING, description: "Full translated text concatenated" },
          blocks: {
            type: Type.ARRAY,
            items: {
              type: Type.OBJECT,
              properties: {
                original: { type: Type.STRING },
                translated: { type: Type.STRING },
                box_2d: {
                  type: Type.ARRAY,
                  items: { type: Type.NUMBER },
                  description: "ymin, xmin, ymax, xmax"
                }
              }
            }
          }
        }
      }
    }
  });

  const text = response.text;
  if (!text) throw new Error("No response from translation service");
  return JSON.parse(text) as TranslationData;
};

/**
 * Analyze an image using Gemini 2.5 Flash with Deep Thinking Mode + ELA Forensic Data
 */
export const analyzeImage = async (
  originalBase64: string, 
  elaBase64: string,
  mimeType: string, 
  lang: Language
): Promise<AnalysisData> => {
  try {
    const langMap: Record<Language, string> = {
      en: "English",
      zh: "Simplified Chinese (zh-CN)",
      es: "Spanish",
      ja: "Japanese",
      fr: "French",
      de: "German",
      pt: "Portuguese"
    };

    const targetLang = langMap[lang] || "English";
    const langInstruction = `The final JSON output values MUST be in ${targetLang}.`;

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: {
        parts: [
          // PART 1: Original Image
          {
            inlineData: {
              mimeType: mimeType,
              data: originalBase64,
            },
          },
          // PART 2: ELA Heatmap (Forensic Evidence)
          {
            inlineData: {
              mimeType: 'image/png',
              data: elaBase64,
            },
          },
          // PART 3: Comprehensive Forensic Instruction
          {
            text: `You are the world's leading Digital Forensic Expert.
            You have two inputs: 1. Original Image. 2. Enhanced ELA (Error Level Analysis) Heatmap.
            
            ${langInstruction}

            Your task is to perform a rigorous "Dual-Track Analysis" to detect both **AI Generation** and **Manual Tampering (Photoshop)**.
            
            ### TRACK 1: DETECTING AI GENERATION (Deepfakes / Midjourney / Stable Diffusion / Flux)
            Scrutinize the image for these specific AI artifacts:
            1. **Unnatural Textures**: Look for "waxy", overly smooth, or "plastic" skin. Look for "painterly" hair that lacks individual strands. Check for noisy patterns in flat areas.
            2. **Inconsistent Lighting**: Check if shadows fall in different directions for different objects. Look for impossible reflections in eyes or mirrors.
            3. **Semantic Inconsistencies**: Check for logical errors. 
               - Hands/Fingers: Are there 6 fingers? Merged fingers? 
               - Text: Is background text gibberish or alien-like?
               - Accessories: Do glasses/earrings match on both sides?
               - Physics: Are objects melting or floating?

            ### TRACK 2: DETECTING MANUAL MANIPULATION (Photoshop / Splice)
            Use the ELA Heatmap:
            1. **The Rainbow Rule**: Authentic images have uniform noise (rainbow speckles) across similar textures.
            2. **Alien Artifacts**: If an object (e.g., a face, sign, or person) is significantly BRIGHTER or has a DIFFERENT COLOR noise pattern than the background, it is a foreign insertion.
            
            Return the analysis in JSON format.`
          }
        ]
      },
      config: {
        thinkingConfig: { thinkingBudget: 12000 },
        maxOutputTokens: 8192,
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            description: { type: Type.STRING },
            tags: { type: Type.ARRAY, items: { type: Type.STRING } },
            objects: { type: Type.ARRAY, items: { type: Type.STRING } },
            sentiment: { type: Type.STRING },
            colors: { type: Type.ARRAY, items: { type: Type.STRING } },
            integrity: {
              type: Type.OBJECT,
              properties: {
                is_suspected_fake: { type: Type.BOOLEAN },
                confidence_score: { type: Type.NUMBER },
                reasoning: { type: Type.STRING },
                methods_analyzed: { type: Type.ARRAY, items: { type: Type.STRING } },
                ai_generated_probability: { type: Type.NUMBER },
                ai_analysis: {
                  type: Type.OBJECT,
                  properties: {
                    unnatural_textures: { type: Type.BOOLEAN },
                    inconsistent_lighting: { type: Type.BOOLEAN },
                    semantic_inconsistencies: { type: Type.BOOLEAN }
                  }
                }
              }
            }
          }
        }
      }
    });

    const text = response.text;
    if (!text) throw new Error("No response from model");
    return JSON.parse(text) as AnalysisData;

  } catch (error) {
    console.error("Gemini Analysis Failed:", error);
    throw error;
  }
};

/**
 * Edit/Modify an image using Gemini 2.5 Flash Image Model
 * Supports both Image-to-Image (if base64 provided) and Text-to-Image (if base64 is null)
 */
export const modifyImage = async (
  base64: string | null, 
  mimeType: string | null, 
  prompt: string
): Promise<string> => {
  try {
    const parts: any[] = [];

    // If image is provided, optimize and add it (Image-to-Image)
    if (base64 && mimeType) {
       const optimizedBase64 = await optimizeImageForApi(base64, mimeType);
       parts.push({
         inlineData: {
           mimeType: 'image/jpeg',
           data: optimizedBase64,
         },
       });
    }

    // Add text prompt
    parts.push({ text: prompt });

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image',
      contents: { parts },
    });

    // The model returns an image in the response parts
    const candidates = response.candidates;
    if (candidates && candidates.length > 0) {
      const parts = candidates[0].content.parts;
      for (const part of parts) {
        if (part.inlineData && part.inlineData.data) {
          return part.inlineData.data;
        }
      }
    }
    
    throw new Error("No image generated in response");
  } catch (error) {
    console.error("Image Modification Failed:", error);
    throw error;
  }
};

/**
 * OCR - Extract Text from Image
 */
export const translateImageText = async (base64: string, mimeType: string, targetLang: string): Promise<TranslationData> => {
  try {
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash',
      contents: {
        parts: [
          { inlineData: { mimeType, data: base64 } },
          { text: `Perform High-Precision OCR on this image. 
                   1. Transcribe the original text exactly as it appears, preserving line breaks.
                   2. Translate the text to ${targetLang}.
                   3. Detect the original language.` }
        ]
      },
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            detected_language: { type: Type.STRING },
            original_text: { type: Type.STRING },
            translated_text: { type: Type.STRING },
          }
        }
      }
    });

    const text = response.text;
    if (!text) throw new Error("No response");
    return JSON.parse(text) as TranslationData;

  } catch (error) {
    console.error("Translation Failed:", error);
    throw error;
  }
};
